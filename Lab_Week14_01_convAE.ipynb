{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab2022_Week14_01_convAE.ipynb","provenance":[{"file_id":"1wCuTWZ97gBQmeEhkBn64E8neu_SdVxRj","timestamp":1612905236869}],"collapsed_sections":["CMryP5VppfhE","Bq9IDbAjHjci","IO2wYhysp6Nc","vO_U8btxp9I0"],"toc_visible":true,"history_visible":true,"authorship_tag":"ABX9TyOUQR1AYwxV+FPeUQrGynX4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CMryP5VppfhE"},"source":["## Convolutional AE"]},{"cell_type":"markdown","source":["This notebook demonstrates how train a Convolutional  Autoencoder (convVAE) on the MNISt dataset.\n","\n","### Your task\n","1.   Learn and summarize convAE with respect to: Architecture, Cost Funstion, Latent Space, Reparameterization, etc..\n","2.   Try to change the dataset to MNISt Denoising and Face-Sketch datasets\n","\n"],"metadata":{"id":"Bq9IDbAjHjci"}},{"cell_type":"markdown","metadata":{"id":"IO2wYhysp6Nc"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OnZLE7b8LA6q","executionInfo":{"status":"ok","timestamp":1644336422173,"user_tz":0,"elapsed":8482,"user":{"displayName":"Deep Perception","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQOCW6zkh1X1hylIK0ktIx1YpiXHevNKSdLkZJ=s64","userId":"15861160368812880821"}},"outputId":"65203ad0-b137-4f74-e1a1-7f21b07da649"},"source":["from keras.datasets import mnist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","(x_train, y_train), (x_test, y_train) = mnist.load_data()\n","\n","x_train = x_train.astype('float32') / 255.\n","x_test = x_test.astype('float32') / 255.\n","x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n","x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n","print(x_train.shape)\n","print(x_test.shape)\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","(60000, 28, 28, 1)\n","(10000, 28, 28, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"vO_U8btxp9I0"},"source":["## Define Conv AE Model"]},{"cell_type":"code","metadata":{"id":"5b0MelFfzGHk","executionInfo":{"status":"ok","timestamp":1644336423084,"user_tz":0,"elapsed":929,"user":{"displayName":"Deep Perception","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiQOCW6zkh1X1hylIK0ktIx1YpiXHevNKSdLkZJ=s64","userId":"15861160368812880821"}}},"source":["import keras\n","from keras import layers\n","\n","input_img = keras.Input(shape=(28, 28, 1))\n","\n","x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n","x = layers.MaxPooling2D((2, 2), padding='same')(x)\n","x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n","x = layers.MaxPooling2D((2, 2), padding='same')(x)\n","x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n","encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n","\n","# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n","\n","x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n","x = layers.UpSampling2D((2, 2))(x)\n","x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n","x = layers.UpSampling2D((2, 2))(x)\n","x = layers.Conv2D(16, (3, 3), activation='relu')(x)\n","x = layers.UpSampling2D((2, 2))(x)\n","decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n","\n","model_convAE = keras.Model(input_img, decoded)\n","\n","# Separable conv encoder\n","model_convAE_encoder = keras.Model(input_img, encoded)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Training and Test"],"metadata":{"id":"Kmna_nVjF5Wa"}},{"cell_type":"markdown","metadata":{"id":"ZPURsh-CzUjX"},"source":["Training"]},{"cell_type":"code","metadata":{"id":"n9mnqvkuzehg"},"source":["from keras.callbacks import TensorBoard\n","\n","model_convAE.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","model_convAE.fit(x_train, x_train,\n","                epochs=10,\n","                batch_size=128,\n","                shuffle=True,\n","                validation_data=(x_test, x_test),\n","                callbacks=[TensorBoard(log_dir='/tmp/model_convAE')])\n","\n","plt.plot(model_convAE.history.history[\"loss\"])\n","plt.plot(model_convAE.history.history[\"val_loss\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2zpj4jXzfj8"},"source":["decoded_imgs = model_convAE.predict(x_test)\n","\n","n = 10\n","plt.figure(figsize=(20, 4))\n","for i in range(1, n + 1):\n","    # Display original\n","    ax = plt.subplot(2, n, i)\n","    plt.imshow(x_test[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","    # Display reconstruction\n","    ax = plt.subplot(2, n, i + n)\n","    plt.imshow(decoded_imgs[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ce5ust38zt1G"},"source":["encoded_imgs = model_convAE_encoder.predict(x_test)\n","\n","n = 10\n","plt.figure(figsize=(20, 8))\n","for i in range(1, n + 1):\n","    ax = plt.subplot(1, n, i)\n","    plt.imshow(encoded_imgs[i].reshape((4, 4 * 8)).T)\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]}]}